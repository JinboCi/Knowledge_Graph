{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KG Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinboCi/Knowledge_Graph/blob/master/KG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B3zuB2NpuxHl"
      },
      "source": [
        "# KG Project\n",
        "\n",
        "Want to look up information about a Nobel Prize Laureate with something built by your own?\n",
        "\n",
        "In this project, we are trying to build a searching system with information storing in knowledge graphs. The mechanism includes three parts:\n",
        "\n",
        "1. Collecting the data from Wikipedia (<https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country>)\n",
        "2. Mapping sturctured data into RDF (please refer to <https://jena.apache.org/tutorials/rdf_api.html> for details)\n",
        "3. A sparql querior (please visit <https://www.w3.org/TR/rdf-sparql-query/> for more information)\n",
        "\n",
        "First of all, let's install the required packages SPARQLWrapper, rdb2rdf and scrapy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-LVGLJdrvZOs",
        "colab": {}
      },
      "source": [
        "!pip install SPARQLWrapper\n",
        "!pip install rdb2rdf\n",
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz9vVoPaddDS",
        "colab_type": "text"
      },
      "source": [
        "Import the modules we will need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yi447rl1Vm2S",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import re\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "import csv\n",
        "import logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GUE_QZ-Rytv_"
      },
      "source": [
        "## The List of Nobel Prize Laureates\n",
        "\n",
        "We would divide our task of building Sparql databases into two parts:\n",
        "\n",
        " - First, we are going to collect our data, we will use the build-in module *scrapy* to crawl the information on the website and then store it to a CSV file. \n",
        " \n",
        " Most of the code in the first part are encrpyted or modified from Kyran Dale, *Data Visualization with Python and JavaScript_ Scrape, Clean, Explore & Transform Your Data-Oâ€™Reilly Media (2016).* Chapter 6.\n",
        " \n",
        " - After that, when building graphs, we would read the data from that CSV file and convert it into Sparql databases. \n",
        " \n",
        " \n",
        " We write the Building-Graphs function separately, in case that sometimes we are directly provided with CSV files. So we only need to build RDF graphs from the existing CSV files rather than crawl the websites.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "US_158DCGDm-"
      },
      "source": [
        "### A simple verison\n",
        "\n",
        "We write this simple version for debugging purpose, but it is a good chance to have a glance at the outline of our data.\n",
        "\n",
        "For this version, we will only consider three kinds of information of the Nobel Prize Winners:\n",
        "- country\n",
        "- name\n",
        "- link_text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9oRPazuIzZqx",
        "colab": {}
      },
      "source": [
        "# nwinners_list_spider.py\n",
        "\n",
        "# A. Define the data to be scraped\n",
        "class NWinnerItem(scrapy.Item):\n",
        "  country = scrapy.Field()\n",
        "  name = scrapy.Field()\n",
        "  link_text = scrapy.Field()\n",
        "\n",
        "  \n",
        "# B Create a named spider\n",
        "class NWinnerSpiderSimp(scrapy.Spider):\n",
        "  \"\"\" Scrapes the country and link-text of the Nobel-winners. \"\"\"\n",
        "  name = 'nwinners_list'\n",
        "  allowed_domains = ['en.wikipedia.org']\n",
        "  start_urls = [\"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country\"]\n",
        "\n",
        "  \n",
        "  # C A parse method to deal with the HTTP response\n",
        "  def parse(self, response):\n",
        "    h3s = response.xpath('//h3')\n",
        "    items = []\n",
        "    for h3 in h3s:\n",
        "      country = h3.xpath('span[@class=\"mw-headline\"]/text()')\\\n",
        "      .extract()\n",
        "      if country:\n",
        "        winners = h3.xpath('following-sibling::ol[1]')\n",
        "        for w in winners.xpath('li'):\n",
        "          text = w.xpath('descendant-or-self::text()')\\\n",
        "          .extract()\n",
        "          items.append(NWinnerItem(\n",
        "            country=country[0], name=text[0],\n",
        "            link_text = ' '.join(text)\n",
        "            ))\n",
        "    return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqJD74x2bofx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(NWinnerSpiderSimp)\n",
        "process.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KGNb_v4s_YFZ"
      },
      "source": [
        "Looks good? However, remember the restart the kernel after running. I don't know why I could run only one process once the kernel starts.\n",
        "\n",
        "Then import all the modules we will need in the following sections:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm4jIWT1dH8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import re\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "import csv\n",
        "import logging\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON, XML\n",
        "\n",
        "import csv\n",
        "import rdflib\n",
        "from rdflib import URIRef, BNode, Literal\n",
        "from rdflib import Namespace\n",
        "from rdflib.namespace import RDF, FOAF\n",
        "from rdflib import Graph, Literal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q9K2GDJJGlD6"
      },
      "source": [
        "### A comprehensive version\n",
        "\n",
        "Besides *country*, *name*, *link_text* that we have previously considered, in this part, we will fetch more information about Nobel Prize Winners, including:\n",
        " - year\n",
        " - category\n",
        " - nationality\n",
        " - gender\n",
        " - born_in\n",
        " - date_of_birth\n",
        " - date_of_death\n",
        " - place_of_birth\n",
        " - place_of_death\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "laX0goPfHWsV",
        "colab": {}
      },
      "source": [
        "BASE_URL = 'http://en.wikipedia.org'\n",
        "class NWinnerItem(scrapy.Item):\n",
        "  name = scrapy.Field()\n",
        "  link = scrapy.Field()\n",
        "  year = scrapy.Field()\n",
        "  category = scrapy.Field()\n",
        "  nationality = scrapy.Field()\n",
        "  gender = scrapy.Field()\n",
        "  born_in = scrapy.Field()\n",
        "  date_of_birth = scrapy.Field()\n",
        "  date_of_death = scrapy.Field()\n",
        "  place_of_birth = scrapy.Field()\n",
        "  place_of_death = scrapy.Field()\n",
        "  text = scrapy.Field()\n",
        "  \n",
        "# B Create a named spider\n",
        "class NWinnerSpiderComph(scrapy.Spider):\n",
        "  \"\"\" Scrapes the country and link-text of the Nobel-winners. \"\"\"\n",
        "  name = 'nwinners_list'\n",
        "  allowed_domains = ['en.wikipedia.org']\n",
        "  start_urls = [\"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country\"]\n",
        "\n",
        "  \n",
        "  # C A parse method to deal with the HTTP response\n",
        "  def parse(self, response):\n",
        "    h3s = response.xpath('//h3')\n",
        "    items = []\n",
        "    for h3 in h3s:\n",
        "      country = h3.xpath('span[@class=\"mw-headline\"]/text()')\\\n",
        "                .extract()\n",
        "      if country:\n",
        "        winners = h3.xpath('following-sibling::ol[1]')\n",
        "        for w in winners.xpath('li'):\n",
        "          wdata = self.process_winner_li(w, country[0])\n",
        "          request = scrapy.Request(\n",
        "            wdata['link'],\n",
        "            callback=self.parse_bio,\n",
        "            dont_filter=True)\n",
        "          request.meta['item'] = NWinnerItem(**wdata)\n",
        "          yield request\n",
        "    return items\n",
        "  def process_winner_li(self, w, country=None):\n",
        "    \"\"\"\n",
        "    Process a winner's <li> tag, adding country of birth or\n",
        "    nationality, as applicable.\n",
        "    \"\"\"\n",
        "    wdata = {}\n",
        "    wdata['link'] = BASE_URL + w.xpath('a/@href').extract()[0]\n",
        "    text = ' '.join(w.xpath('descendant-or-self::text()').extract())\n",
        "    # get comma-delineated name and strip trailing white-space\n",
        "    wdata['name'] = text.split(',')[0].strip()\n",
        "    # see if there are four adjecent integers in the string text\n",
        "    year = re.findall('\\d{4}', text)\n",
        "    if year:\n",
        "      wdata['year'] = int(year[0])\n",
        "    else:\n",
        "      wdata['year'] = 0\n",
        "      print('Oops, no year in ', text)\n",
        "    category = re.findall('Physics|Chemistry|Physiology or Medicine|Literature|Peace|Economics',text)\n",
        "    if category:\n",
        "      wdata['category'] = category[0]\n",
        "    else:\n",
        "      wdata['category'] = ''\n",
        "      print('Oops, no category in ', text)\n",
        "    if country:\n",
        "      # the interesting label that represent his nationality or motherland\n",
        "      if text.find('*') != -1:\n",
        "        wdata['nationality'] = ''\n",
        "        wdata['born_in'] = country\n",
        "      else:\n",
        "        wdata['nationality'] = country\n",
        "        wdata['born_in'] = ''\n",
        "    # store a copy of the link's text-string for any manual corrections\n",
        "    wdata['text'] = text\n",
        "    return wdata\n",
        "  \n",
        "  def parse_bio(self, response):\n",
        "    item = response.meta['item']\n",
        "    href = response.xpath(\"//li[@id='t-wikibase']/a/@href\").extract()\n",
        "    if href:\n",
        "      request = scrapy.Request(href[0],\\\n",
        "                  callback=self.parse_wikidata,\\\n",
        "                              dont_filter=True)\n",
        "      request.meta['item'] = item\n",
        "      yield request\n",
        "  def parse_wikidata(self, response):\n",
        "    item = response.meta['item']\n",
        "    property_codes = [\n",
        "      {'name':'date_of_birth', 'code':'P569'},\n",
        "      {'name':'date_of_death', 'code':'P570'},\n",
        "      {'name':'place_of_birth', 'code':'P19', 'link':True},\n",
        "      {'name':'place_of_death', 'code':'P20', 'link':True},\n",
        "      {'name':'gender', 'code':'P21', 'link':True}\n",
        "    ]    \n",
        "    p_template = '//*[@id=\"%(code)s\"]/div[2]/div[1]/div/div[2]/div[2]/div[1]'\n",
        "    for prop in property_codes:\n",
        "      extra_html = ''\n",
        "      if prop.get('link'): # property string in <a> tag\n",
        "        extra_html = '/a'\n",
        "      sel = response.xpath(p_template%prop + extra_html + '/text()')\n",
        "      if sel:\n",
        "        item[prop['name']] = sel[0].extract()\n",
        "    yield item\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6N9qLAddoNB",
        "colab_type": "text"
      },
      "source": [
        "We will not run this comprehensive version at present since it would print out all the loggings on the screen. But we will run a really similar version in the next part where we will ignore the clumsy loggings and store what we crawl into a CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8F8JScsZ7-A"
      },
      "source": [
        "### Storing to CSV\n",
        "We may want to write the results to csv files or directly store it as RDF graphs. How could we achieve this? \n",
        "\n",
        "Hmm...\n",
        "\n",
        "We may first store the data to CSV, and then upload the CSV file to our sql server!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aZGiCQMKZ48I",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "BASE_URL = 'http://en.wikipedia.org'\n",
        "class NWinnerItem(scrapy.Item):\n",
        "  name = scrapy.Field()\n",
        "  link = scrapy.Field()\n",
        "  year = scrapy.Field()\n",
        "  category = scrapy.Field()\n",
        "  nationality = scrapy.Field()\n",
        "  gender = scrapy.Field()\n",
        "  born_in = scrapy.Field()\n",
        "  date_of_birth = scrapy.Field()\n",
        "  date_of_death = scrapy.Field()\n",
        "  place_of_birth = scrapy.Field()\n",
        "  place_of_death = scrapy.Field()\n",
        "  text = scrapy.Field()\n",
        "  \n",
        "# B Create a named spider\n",
        "class NWinnerSpiderToCsv(scrapy.Spider):\n",
        "  \"\"\" Scrapes the country and link-text of the Nobel-winners. \"\"\"\n",
        "  name = 'nwinners_list'\n",
        "  allowed_domains = ['en.wikipedia.org']\n",
        "  start_urls = [\"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country\"]\n",
        "  items = []\n",
        "  output = name+\".csv\" \n",
        "  custom_settings = {\n",
        "      'LOG_LEVEL': 'INFO',\n",
        "      'FEED_FORMAT':'csv',\n",
        "      'FEED_URI': 'nwinners_list.csv'\n",
        "    }\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    if os.path.isfile(self.output):\n",
        "      os.remove(self.output)\n",
        "    open(self.output, \"w\").close()\n",
        "  '''\n",
        "\n",
        "  \n",
        "  # C A parse method to deal with the HTTP response\n",
        "  def parse(self, response):\n",
        "    h3s = response.xpath('//h3')\n",
        "    for h3 in h3s:\n",
        "      country = h3.xpath('span[@class=\"mw-headline\"]/text()')\\\n",
        "                .extract()\n",
        "      if country:\n",
        "        winners = h3.xpath('following-sibling::ol[1]')\n",
        "        for w in winners.xpath('li'):\n",
        "          wdata = self.process_winner_li(w, country[0])\n",
        "          #pdb.set_trace()\n",
        "          request = scrapy.Request(\n",
        "            wdata['link'],\n",
        "            callback=self.parse_bio,\n",
        "            dont_filter=True)\n",
        "          #pdb.set_trace()\n",
        "          request.meta['item'] = NWinnerItem(**wdata)\n",
        "          yield request\n",
        "          #pdb.set_trace()\n",
        "    #pdb.set_trace()\n",
        "    return self.items\n",
        "  \n",
        "      \n",
        "  def process_winner_li(self, w, country=None):\n",
        "    \"\"\"\n",
        "    Process a winner's <li> tag, adding country of birth or\n",
        "    nationality, as applicable.\n",
        "    \"\"\"\n",
        "    wdata = {}\n",
        "    wdata['link'] = BASE_URL + w.xpath('a/@href').extract()[0]\n",
        "    text = ' '.join(w.xpath('descendant-or-self::text()').extract())\n",
        "    # get comma-delineated name and strip trailing white-space\n",
        "    wdata['name'] = text.split(',')[0].strip()\n",
        "    # see if there are four adjecent integers in the string text\n",
        "    year = re.findall('\\d{4}', text)\n",
        "    if year:\n",
        "      wdata['year'] = int(year[0])\n",
        "    else:\n",
        "      wdata['year'] = 0\n",
        "      print('Oops, no year in ', text)\n",
        "    category = re.findall('Physics|Chemistry|Physiology or Medicine|Literature|Peace|Economics',text)\n",
        "    if category:\n",
        "      wdata['category'] = category[0]\n",
        "    else:\n",
        "      wdata['category'] = ''\n",
        "      print('Oops, no category in ', text)\n",
        "    if country:\n",
        "      # the interesting label that represent his nationality or motherland\n",
        "      if text.find('*') != -1:\n",
        "        wdata['nationality'] = ''\n",
        "        wdata['born_in'] = country\n",
        "      else:\n",
        "        wdata['nationality'] = country\n",
        "        wdata['born_in'] = ''\n",
        "    # store a copy of the link's text-string for any manual corrections\n",
        "    wdata['text'] = text\n",
        "    #pdb.set_trace()\n",
        "    return wdata\n",
        "  \n",
        "  def parse_bio(self, response):\n",
        "    #pdb.set_trace()\n",
        "    item = response.meta['item']\n",
        "    href = response.xpath(\"//li[@id='t-wikibase']/a/@href\").extract()\n",
        "    if href:\n",
        "      request = scrapy.Request(href[0],\\\n",
        "                  callback=self.parse_wikidata,\\\n",
        "                              dont_filter=True)\n",
        "      request.meta['item'] = item\n",
        "      return request\n",
        "  def parse_wikidata(self, response):\n",
        "    #pdb.set_trace()\n",
        "    item = response.meta['item']\n",
        "    property_codes = [\n",
        "      {'name':'date_of_birth', 'code':'P569'},\n",
        "      {'name':'date_of_death', 'code':'P570'},\n",
        "      {'name':'place_of_birth', 'code':'P19', 'link':True},\n",
        "      {'name':'place_of_death', 'code':'P20', 'link':True},\n",
        "      {'name':'gender', 'code':'P21', 'link':True}\n",
        "    ]\n",
        "    # this template should be obtained by carefully examining the webpage's elements\n",
        "    p_template = '//*[@id=\"%(code)s\"]/div[2]/div/div/div[2]/div[1]/div/div[2]/div[2]/div[1]'\n",
        "    for prop in property_codes:\n",
        "      extra_html = ''\n",
        "      if prop.get('link'): # property string in <a> tag\n",
        "        extra_html = '/a'\n",
        "      \n",
        "      sel = response.xpath(p_template%prop + extra_html + '/text()')\n",
        "      #pdb.set_trace()\n",
        "      if sel:\n",
        "        item[prop['name']] = sel[0].extract()\n",
        "      else:\n",
        "        item[prop['name']] = \"\"\n",
        "    self.items.append(NWinnerItem(name=item[\"name\"], link=item[\"link\"], \n",
        "                                   year=item[\"year\"], category=item[\"category\"],\n",
        "                                  nationality=item[\"nationality\"], gender=item[\"gender\"],\n",
        "                                  born_in=item[\"born_in\"], date_of_birth=item[\"date_of_birth\"],\n",
        "                                  date_of_death=item[\"date_of_death\"],\n",
        "                                  place_of_birth=item[\"place_of_birth\"],\n",
        "                                  place_of_death=item[\"place_of_death\"],\n",
        "                                  text=item[\"text\"]))\n",
        "    return item\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QED04UTHXNrk"
      },
      "source": [
        "Now, let's try to see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zddzchmuJz3Z",
        "colab": {}
      },
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(NWinnerSpiderToCsv)\n",
        "process.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2BDMQxB3vcek"
      },
      "source": [
        "## Building Graphs\n",
        "\n",
        "The code in this part is to generate RDF graphs from datasets. For the simplicity, we would only consider the simplest case - Mapping structured data into graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPLj4T58x5H8"
      },
      "source": [
        "### Structured datasets\n",
        "\n",
        "In this part, we would build our RDF graphs from the structured datasets (i.e. CSV files). We would achieve it by applying the libraries *rdflib* and *csv*. \n",
        "\n",
        "This function would receive 3 three parameters:\n",
        "\n",
        "- filepath: the path of our CSV file\n",
        "- output : the expected path of our output file\n",
        "- output_format: the output format, including 'xml', 'n3', 'turtle', 'nt', 'pretty-xml', 'trix', 'trig' and 'nquads'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2Mh-BbKuxHm",
        "colab": {}
      },
      "source": [
        "def R2RDF(filepath, output, output_format):\n",
        "  first_Row = True\n",
        "  graph = Graph()\n",
        "  if os.path.isfile(filepath):\n",
        "    with open(filepath, encoding=\"utf-8\") as csvfile:\n",
        "      readCSV = csv.reader(csvfile,delimiter=',')\n",
        "      wiki_prefix = Namespace('https://en.wikipedia.org/wiki/')\n",
        "      for row in readCSV:\n",
        "        #print(row)\n",
        "        if first_Row == True:\n",
        "          first_Row = False\n",
        "          continue\n",
        "        born_in = Literal(row[0])\n",
        "        category = Literal(row[1])\n",
        "        date_of_birth = Literal(row[2])\n",
        "        date_of_death = Literal(row[3])\n",
        "        gender = Literal(row[4])\n",
        "        link = Literal(row[5])\n",
        "        name = Literal(row[6])\n",
        "        nationality = Literal(row[7])\n",
        "        place_of_birth = Literal(row[8])\n",
        "        place_of_death = Literal(row[9])\n",
        "        text = Literal(row[10])\n",
        "        year = Literal(row[11])\n",
        "        current_node = URIRef(link)\n",
        "        graph.add((current_node, RDF.type, FOAF.Person))\n",
        "        graph.add((current_node, FOAF.name, name))\n",
        "        graph.add((current_node, wiki_prefix.homeland, born_in))\n",
        "        #if born_in != \"\":\n",
        "        #graph.add((born_in, RDF.type, wiki_prefix.country))\n",
        "        graph.add((current_node, wiki_prefix.Course_education, category))\n",
        "        graph.add((current_node, wiki_prefix.date_of_birth,date_of_birth))\n",
        "        #graph.add((date_of_birth, RDF.type, FOAF.Date))\n",
        "        #if date_of_death != \"\":\n",
        "          #graph.add((date_of_death, RDF.type, FOAF.Date))\n",
        "        graph.add((current_node, wiki_prefix.date_of_death, date_of_death))\n",
        "        #if gender != \"\":\n",
        "        graph.add((current_node, FOAF.gender, gender))\n",
        "        graph.add((current_node, FOAF.accountServiceHomepage, link))\n",
        "        graph.add((current_node, wiki_prefix.nationality, nationality))\n",
        "        #graph.add((nationality, RDF.type, wiki_prefix.country))\n",
        "        #if place_of_birth != \"\":\n",
        "        graph.add((current_node, wiki_prefix.place_of_birth, place_of_birth))\n",
        "        #graph.add((place_of_birth, RDF.type, wiki_prefix.city))\n",
        "        #if place_of_death != \"\":\n",
        "        graph.add((current_node, wiki_prefix.place_of_death, place_of_death))\n",
        "        #graph.add((place_of_death, RDF.type, wiki_prefix.city))\n",
        "        graph.add((current_node, FOAF.depiction, text))\n",
        "        graph.add((current_node, wiki_prefix.year, year))\n",
        "    if os.path.isfile(output):\n",
        "        os.remove(output)\n",
        "    graph.serialize(destination=output, format=output_format)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSta8lijV-OY",
        "colab_type": "text"
      },
      "source": [
        "Now let's look how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h91POpQsV-Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R2RDF('nwinners_list.csv', 'nwinners_list.ttl', 'turtle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Tjcrq6V-Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R2RDF('nwinners_list.csv', 'nwinners_list.xml', 'pretty-xml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "loCdcKNLuxHo"
      },
      "source": [
        "## Sparql Querior\n",
        "\n",
        "We are writing a class that is able to achieve basic implementations of the SPQRQL queries for an RDF database. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIhw_amOV-Of",
        "colab_type": "text"
      },
      "source": [
        "### Querying with SPARQLWrapper (probably failed)\n",
        "The following functions are to be considered:\n",
        "\n",
        "- Function *init*\n",
        "- Function *Querying_database* \n",
        "\n",
        "The Fit_project and its member functions would take the inputs: \n",
        "\n",
        "- Url: the path of the RDF graph\n",
        "\n",
        "- Enum: the indication that where the *Input_stirng* is the path of an input file or the sparql request\n",
        "\n",
        "- Return_format: JSON or XML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rl5YbW3CuxHr",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "class Query_with_SPARQLWrapper:\n",
        "    File_or_Query = {\n",
        "        \"File\",\n",
        "        \"Query\",\n",
        "    }\n",
        "    Return_format = {\n",
        "        \"JSON\": JSON,\n",
        "        \"XML\": XML,\n",
        "    }\n",
        "    File_path = \"\"\n",
        "    Query_string = \"\"\n",
        "    Url = \"\"\n",
        "    Sparql = SPARQLWrapper(\"\")\n",
        "    def __init__(self, Url):\n",
        "        self.Sparql = SPARQLWrapper(Url)\n",
        "    def Querying_database(self, Enum, Input_string, Return_format):\n",
        "        if Enum == \"File\":\n",
        "            self.File_path = Input_string\n",
        "            if not os.path.isfile(self.File_path):\n",
        "                raise TypeError(self.File_path + \" does not exist\")\n",
        "            self.Query_string = open(self.File_path).read().close()\n",
        "        else:\n",
        "            self.Query_string = Input_string\n",
        "        self.Sparql.setQuery(self.Query_string)\n",
        "        #pdb.set_trace()\n",
        "        if Return_format == \"JSON\":\n",
        "            self.Sparql.setReturnFormat(JSON)\n",
        "        else:\n",
        "            self.Sparql.setReturnFormat(XML)\n",
        "        results = self.Sparql.query().convert()\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrILb-wyV-Oj",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62rLYv2jV-Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fp = Fit_project(\"https://raw.githubusercontent.com/JinboCi/Knowledge_Graph/master/nwinners_list.xml\")\n",
        "print(Fp.Querying_database(\"Query\", \n",
        "                           \"\"\"SELECT ?subject ?predicate \n",
        "WHERE {\n",
        "  ?subject ?predicate \"Chicago\"\n",
        "}\n",
        "LIMIT 25\"\"\", \n",
        "                           \"JSON\").decode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a57u1-zoV-On",
        "colab_type": "text"
      },
      "source": [
        "### Rdflib method\n",
        "\n",
        "OMG, we have failed to use SPARQLWrapper for querying. Sadly but hopefully, there is a another way!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1DWor24V-Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Rdflib_method(filepath, inputstring):\n",
        "    graph = rdflib.Graph()\n",
        "    graph.parse(filepath, format='xml')\n",
        "    qres = graph.query(inputstring)\n",
        "    for row in qres:\n",
        "        print(row)\n",
        "    return qres"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKMDTjbIV-Oq",
        "colab_type": "text"
      },
      "source": [
        "How it works?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEhEVS9dV-Or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5febb7cb-c2ba-4453-e26d-9f9a5d96ad8e"
      },
      "source": [
        "Rdflib_method(\"https://raw.githubusercontent.com/JinboCi/Knowledge_Graph/master/nwinners_list.xml\", \n",
        "             \"\"\"\n",
        "             PREFIX  foaf: <http://xmlns.com/foaf/0.1/>\n",
        "SELECT ?object \n",
        "WHERE {\n",
        "  ?subject ?predicate \"Tibet\".\n",
        "  ?subject foaf:name ?object\n",
        "}\n",
        "\"\"\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(rdflib.term.Literal('14th Dalai Lama'),)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<rdflib.plugins.sparql.processor.SPARQLResult at 0x7f5d573ac278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RryE7S90hkWZ",
        "colab_type": "text"
      },
      "source": [
        "Yes! We build this knowledge graph system successfully!"
      ]
    }
  ]
}
