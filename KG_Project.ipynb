{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KG Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinboCi/Knowledge_Graph/blob/master/KG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B3zuB2NpuxHl"
      },
      "source": [
        "# KG Project\n",
        "\n",
        "I am building a Fuseki Server and a Sparql Querior to implement Knowledge Graph.\n",
        "\n",
        "First of all, let's install the required packages SPARQLWrapper, rdb2rdf and scrapy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-LVGLJdrvZOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e2a3c81-c350-4802-fdf4-7079029ce189"
      },
      "source": [
        "!pip install SPARQLWrapper\n",
        "!pip install rdb2rdf\n",
        "!pip install scrapy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SPARQLWrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/1d/d7c60a451a255fca655fe37eb3f6e3b3daa7d33fc87eeec0d8631d501e76/SPARQLWrapper-1.8.4-py3-none-any.whl\n",
            "Collecting rdflib>=4.0 (from SPARQLWrapper)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.0->SPARQLWrapper) (2.4.2)\n",
            "Collecting isodate (from rdflib>=4.0->SPARQLWrapper)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from isodate->rdflib>=4.0->SPARQLWrapper) (1.12.0)\n",
            "Installing collected packages: isodate, rdflib, SPARQLWrapper\n",
            "Successfully installed SPARQLWrapper-1.8.4 isodate-0.6.0 rdflib-4.2.2\n",
            "Collecting rdb2rdf\n",
            "  Downloading https://files.pythonhosted.org/packages/04/20/ec65e461969abbe4955d2bdef070872f895706af09d07f730f811ff6ee16/rdb2rdf-0.1.3.tar.gz\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from rdb2rdf) (4.2.2)\n",
            "Collecting spruce-collections (from rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/32/bd/64c9598819e22800e4a87f7f6b922d1c8cfe9070ff8b9550e23a8f3f1e5a/Spruce-collections-0.2.3.tar.gz\n",
            "Collecting spruce-datetime (from rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/37/07/42aa33a8ce04bab0929e1406db1dbb318114b453fed7b4b09af10ef08fa7/Spruce-datetime-0.2.2.tar.gz\n",
            "Collecting spruce-iri (from rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/37/10/ebc0f20c310ff36bb497a9b15023561e8a03d34e164ccc43982fb215da3f/Spruce-iri-0.2.0.tar.gz\n",
            "Collecting spruce-types (from rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/64/83/fa338b569aaeaaeac36d5b146d3049abd3119fa09fba4a5e4346ec74c6f1/Spruce-types-0.1.1.tar.gz\n",
            "Requirement already satisfied: sqlalchemy>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from rdb2rdf) (1.3.6)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->rdb2rdf) (2.4.2)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->rdb2rdf) (0.6.0)\n",
            "Collecting multimap (from spruce-collections->rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/e3/a671749ea7122203c17d68edb99fa105b0a54c545b1fe7de5afd915038e1/MultiMap-1.0.3.tar.gz\n",
            "Collecting oset (from spruce-collections->rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/b1/a49498c699a3fda5d635cc1fa222ffc686ea3b5d04b84a3166c4cab0c57b/oset-0.1.3.tar.gz\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from spruce-datetime->rdb2rdf) (2018.9)\n",
            "Collecting goosetypes (from spruce-iri->rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/09/91f72299ae60d84f03db969e6bd09575fa3b6ff20d39cdbc63a07d6e3ac6/GooseTypes-0.1.1.tar.gz\n",
            "Collecting spruce-exc (from spruce-types->rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/9c/4029fdbb35c275c6b4c53a192b6eb9c7208ee2f0a5953c182eb8f03502d4/Spruce-exc-0.1.0.tar.gz\n",
            "Collecting spruce-lang (from spruce-types->rdb2rdf)\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/74/3cb7abe0e0031f50ccbdca9369aae0569e4f007cab1d631d7c19e1f21909/Spruce-lang-0.2.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from isodate->rdflib->rdb2rdf) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from oset->spruce-collections->rdb2rdf) (41.0.1)\n",
            "Building wheels for collected packages: rdb2rdf, spruce-collections, spruce-datetime, spruce-iri, spruce-types, multimap, oset, goosetypes, spruce-exc, spruce-lang\n",
            "  Building wheel for rdb2rdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rdb2rdf: filename=rdb2rdf-0.1.3-cp36-none-any.whl size=19779 sha256=f53406061a4ceba8e4d65ca8504f37412c706f121e1425b3090d5a74c6ba54c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/2b/f1/7c9676254995cbfe394521d2e6cf41d3b959c8fc0547f66d0c\n",
            "  Building wheel for spruce-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spruce-collections: filename=Spruce_collections-0.2.3-cp36-none-any.whl size=16821 sha256=127a7e64a0492f8c685e641f75e8b13ad5c156d8b88296e6ea4e3b93d4a71e52\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/32/92/c3e01a9d6afd66c7fad95a6f967543027bf68e82e491bd14fe\n",
            "  Building wheel for spruce-datetime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spruce-datetime: filename=Spruce_datetime-0.2.2-cp36-none-any.whl size=14793 sha256=ade844c959e7cd5780aa8b036bb0ccfafe5b21dc137740b183dfd45f755747a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/7d/e9/e1838fa92057edd5ea2967e335a50444091fd4b8308aca8aac\n",
            "  Building wheel for spruce-iri (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spruce-iri: filename=Spruce_iri-0.2.0-cp36-none-any.whl size=10715 sha256=98f2d47f35c8c7fdae814c4f3dbc97dd90981ff8b09aeaaf60c3a71bec9a9a13\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/77/09/8708f7614cc831aeedaa234773dddb3c0f382dbd028043278b\n",
            "  Building wheel for spruce-types (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spruce-types: filename=Spruce_types-0.1.1-cp36-none-any.whl size=16812 sha256=e9cf76fc964424b40a75c8d86e120f7a2827b38f06e4bf233abe6e4fa8b43c37\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/8f/3a/5a2c43335627edd0a26afd09dcef91389d3f950b099a5c6533\n",
            "  Building wheel for multimap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for multimap: filename=MultiMap-1.0.3-cp36-none-any.whl size=6134 sha256=a8bbcf8cf6f04eea591e2df285c505606815f63837ce78bb22d8d4e71f8e345d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/0e/76/3084879a2e58cad344bbdae451dd9d87249baa8b8a890b6d92\n",
            "  Building wheel for oset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oset: filename=oset-0.1.3-cp36-none-any.whl size=9661 sha256=3194075ac7bf16087d64b10c27d9b03b8ebf36ef929b1cbaeb85aae45ffa4294\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/87/c8/3dad2dca279f64fb68af5d9908c380fee2f16488a1b1da3499\n",
            "  Building wheel for goosetypes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for goosetypes: filename=GooseTypes-0.1.1-cp36-none-any.whl size=12246 sha256=e8500294397c9014a913cd06073645a023ad901a4f91d23693238df4c27e02ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/78/a7/f9010f65ed8b17c00ca642cafde30bdf71c0bda6610945292a\n",
            "  Building wheel for spruce-exc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spruce-exc: filename=Spruce_exc-0.1.0-cp36-none-any.whl size=8795 sha256=91d03bade63895f52ec7790d052ba8b3b1b1b80c4fac449861aa00fd0eccfab8\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/de/14/ef693bf51821bc90174758c2b352ec933aec798577775b4545\n",
            "  Building wheel for spruce-lang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spruce-lang: filename=Spruce_lang-0.2.0-cp36-none-any.whl size=8777 sha256=75001ce791a93cf5952b2e2f93421defd3b8f7964650ef58e4d1b842c275b77c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/b5/12/856662366d56bae7fbd5917eaef31d48d76c736e246e940a7a\n",
            "Successfully built rdb2rdf spruce-collections spruce-datetime spruce-iri spruce-types multimap oset goosetypes spruce-exc spruce-lang\n",
            "Installing collected packages: multimap, oset, spruce-exc, spruce-lang, spruce-types, spruce-collections, spruce-datetime, goosetypes, spruce-iri, rdb2rdf\n",
            "Successfully installed goosetypes-0.1.1 multimap-1.0.3 oset-0.1.3 rdb2rdf-0.1.3 spruce-collections-0.2.3 spruce-datetime-0.2.2 spruce-exc-0.1.0 spruce-iri-0.2.0 spruce-lang-0.2.0 spruce-types-0.1.1\n",
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/4b/585e8e111ffb01466c59281f34febb13ad1a95d7fb3919fd57c33fc732a5/Scrapy-1.7.3-py2.py3-none-any.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 2.8MB/s \n",
            "\u001b[?25hCollecting service-identity (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting pyOpenSSL (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 23.3MB/s \n",
            "\u001b[?25hCollecting queuelib (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting cssselect>=0.9 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
            "Collecting parsel>=1.5 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/96/69/d1d5dba5e4fecd41ffd71345863ed36a45975812c06ba77798fc15db6a64/parsel-1.5.1-py2.py3-none-any.whl\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting Twisted>=13.1.0; python_version != \"3.4\" (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/49/eb654da38b15285d1f594933eefff36ce03106356197dba28ee8f5721a79/Twisted-19.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 42.6MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/81/43/9dcf92a77f5f0afe4f4df2407d7289eea01368a08b64bda00dd318ca62a6/w3lib-1.20.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.12.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.2.6)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (19.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.4.6)\n",
            "Collecting cryptography (from service-identity->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 38.0MB/s \n",
            "\u001b[?25hCollecting hyperlink>=17.1.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.4.2 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/17/1d198a6aaa9aa4590862fe3d3a2ed7dd808050cab4eebe8a2f2f813c1376/zope.interface-4.6.0-cp36-cp36m-manylinux1_x86_64.whl (167kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 47.0MB/s \n",
            "\u001b[?25hCollecting incremental>=16.10.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest>=1.9.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 25.6MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting Automat>=0.3.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->service-identity->scrapy) (1.12.3)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography->service-identity->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (41.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->service-identity->scrapy) (2.19)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11517 sha256=829e91966b1e4ee2fb72c05201b5c774e2d87c8d1c2f0b1d934252ad53dcd1bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: asn1crypto, cryptography, service-identity, pyOpenSSL, queuelib, cssselect, w3lib, parsel, PyDispatcher, hyperlink, zope.interface, incremental, PyHamcrest, constantly, Automat, Twisted, scrapy\n",
            "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-19.7.0 asn1crypto-0.24.0 constantly-15.1.0 cryptography-2.7 cssselect-1.0.3 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.1 pyOpenSSL-19.0.0 queuelib-1.5.0 scrapy-1.7.3 service-identity-18.1.0 w3lib-1.20.0 zope.interface-4.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz9vVoPaddDS",
        "colab_type": "text"
      },
      "source": [
        "Import the modules we will need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yi447rl1Vm2S",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import re\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "import csv\n",
        "import logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GUE_QZ-Rytv_"
      },
      "source": [
        "## Collecting data\n",
        "\n",
        "We would divide our task of building Sparql databases into two parts:\n",
        "\n",
        " - First, we are going to collect our data, we will use the build-in module *scrapy* to crawl the information on the website and then store it to a CSV file. \n",
        " - After that, when building graphs, we would read the data from that CSV file and convert it into Sparql databases. \n",
        " \n",
        " \n",
        " We write the Building-Graphs function separately, in case that sometimes we are directly provided with CSV files. So we only need to build RDF graphs from the existing CSV files rather than crawl the websites.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "US_158DCGDm-"
      },
      "source": [
        "### A simple verison\n",
        "\n",
        "We write this simple version for debugging purpose, but it is a good chance to have a glance at the outline.\n",
        "\n",
        "For this version, we will only consider three kinds of information of the Nobel Prize Winners:\n",
        "- country\n",
        "- name\n",
        "- link_text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9oRPazuIzZqx",
        "colab": {}
      },
      "source": [
        "# nwinners_list_spider.py\n",
        "\n",
        "# A. Define the data to be scraped\n",
        "class NWinnerItem(scrapy.Item):\n",
        "  country = scrapy.Field()\n",
        "  name = scrapy.Field()\n",
        "  link_text = scrapy.Field()\n",
        "\n",
        "  \n",
        "# B Create a named spider\n",
        "class NWinnerSpiderSimp(scrapy.Spider):\n",
        "  \"\"\" Scrapes the country and link-text of the Nobel-winners. \"\"\"\n",
        "  name = 'nwinners_list'\n",
        "  allowed_domains = ['en.wikipedia.org']\n",
        "  start_urls = [\"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country\"]\n",
        "\n",
        "  \n",
        "  # C A parse method to deal with the HTTP response\n",
        "  def parse(self, response):\n",
        "    h3s = response.xpath('//h3')\n",
        "    items = []\n",
        "    for h3 in h3s:\n",
        "      country = h3.xpath('span[@class=\"mw-headline\"]/text()')\\\n",
        "      .extract()\n",
        "      if country:\n",
        "        winners = h3.xpath('following-sibling::ol[1]')\n",
        "        for w in winners.xpath('li'):\n",
        "          text = w.xpath('descendant-or-self::text()')\\\n",
        "          .extract()\n",
        "          items.append(NWinnerItem(\n",
        "            country=country[0], name=text[0],\n",
        "            link_text = ' '.join(text)\n",
        "            ))\n",
        "    return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqJD74x2bofx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "503ac9d2-95fc-4820-e8a9-64e757974ad1"
      },
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(NWinnerSpiderSimp)\n",
        "process.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-09 08:13:34 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
            "2019-08-09 08:13:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.7.0, Python 3.6.8 (default, Jan 14 2019, 11:02:34) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2019-08-09 08:13:34 [scrapy.crawler] INFO: Overridden settings: {}\n",
            "2019-08-09 08:13:34 [scrapy.extensions.telnet] INFO: Telnet Password: dc838155db3a99b0\n",
            "2019-08-09 08:13:34 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2019-08-09 08:13:34 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2019-08-09 08:13:34 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2019-08-09 08:13:34 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2019-08-09 08:13:34 [scrapy.core.engine] INFO: Spider opened\n",
            "2019-08-09 08:13:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2019-08-09 08:13:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ReactorNotRestartable",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-44ff82ede848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNWinnerSpiderSimp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shutdown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \"\"\"\n\u001b[1;32m   1261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReactorNotRestartable\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KGNb_v4s_YFZ"
      },
      "source": [
        "Looks good? However, remember the restart the kernel after running. I don't know why I could run only one process once the kernel starts.\n",
        "\n",
        "Then import all the modules we will need in the following sections:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm4jIWT1dH8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import re\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "import csv\n",
        "import logging\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON, XML\n",
        "\n",
        "import csv\n",
        "import rdflib\n",
        "from rdflib import URIRef, BNode, Literal\n",
        "from rdflib import Namespace\n",
        "from rdflib.namespace import RDF, FOAF\n",
        "from rdflib import Graph, Literal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q9K2GDJJGlD6"
      },
      "source": [
        "### A comprehensive version\n",
        "\n",
        "Besides *country*, *name*, *link_text* that we have previously considered, in this part, we will fetch more information about Nobel Prize Winners, including:\n",
        " - year\n",
        " - category\n",
        " - nationality\n",
        " - gender\n",
        " - born_in\n",
        " - date_of_birth\n",
        " - date_of_death\n",
        " - place_of_birth\n",
        " - place_of_death\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "laX0goPfHWsV",
        "colab": {}
      },
      "source": [
        "BASE_URL = 'http://en.wikipedia.org'\n",
        "class NWinnerItem(scrapy.Item):\n",
        "  name = scrapy.Field()\n",
        "  link = scrapy.Field()\n",
        "  year = scrapy.Field()\n",
        "  category = scrapy.Field()\n",
        "  nationality = scrapy.Field()\n",
        "  gender = scrapy.Field()\n",
        "  born_in = scrapy.Field()\n",
        "  date_of_birth = scrapy.Field()\n",
        "  date_of_death = scrapy.Field()\n",
        "  place_of_birth = scrapy.Field()\n",
        "  place_of_death = scrapy.Field()\n",
        "  text = scrapy.Field()\n",
        "  \n",
        "# B Create a named spider\n",
        "class NWinnerSpiderComph(scrapy.Spider):\n",
        "  \"\"\" Scrapes the country and link-text of the Nobel-winners. \"\"\"\n",
        "  name = 'nwinners_list'\n",
        "  allowed_domains = ['en.wikipedia.org']\n",
        "  start_urls = [\"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country\"]\n",
        "\n",
        "  \n",
        "  # C A parse method to deal with the HTTP response\n",
        "  def parse(self, response):\n",
        "    h3s = response.xpath('//h3')\n",
        "    items = []\n",
        "    for h3 in h3s:\n",
        "      country = h3.xpath('span[@class=\"mw-headline\"]/text()')\\\n",
        "                .extract()\n",
        "      if country:\n",
        "        winners = h3.xpath('following-sibling::ol[1]')\n",
        "        for w in winners.xpath('li'):\n",
        "          wdata = self.process_winner_li(w, country[0])\n",
        "          request = scrapy.Request(\n",
        "            wdata['link'],\n",
        "            callback=self.parse_bio,\n",
        "            dont_filter=True)\n",
        "          request.meta['item'] = NWinnerItem(**wdata)\n",
        "          yield request\n",
        "    return items\n",
        "  def process_winner_li(self, w, country=None):\n",
        "    \"\"\"\n",
        "    Process a winner's <li> tag, adding country of birth or\n",
        "    nationality, as applicable.\n",
        "    \"\"\"\n",
        "    wdata = {}\n",
        "    wdata['link'] = BASE_URL + w.xpath('a/@href').extract()[0]\n",
        "    text = ' '.join(w.xpath('descendant-or-self::text()').extract())\n",
        "    # get comma-delineated name and strip trailing white-space\n",
        "    wdata['name'] = text.split(',')[0].strip()\n",
        "    # see if there are four adjecent integers in the string text\n",
        "    year = re.findall('\\d{4}', text)\n",
        "    if year:\n",
        "      wdata['year'] = int(year[0])\n",
        "    else:\n",
        "      wdata['year'] = 0\n",
        "      print('Oops, no year in ', text)\n",
        "    category = re.findall('Physics|Chemistry|Physiology or Medicine|Literature|Peace|Economics',text)\n",
        "    if category:\n",
        "      wdata['category'] = category[0]\n",
        "    else:\n",
        "      wdata['category'] = ''\n",
        "      print('Oops, no category in ', text)\n",
        "    if country:\n",
        "      # the interesting label that represent his nationality or motherland\n",
        "      if text.find('*') != -1:\n",
        "        wdata['nationality'] = ''\n",
        "        wdata['born_in'] = country\n",
        "      else:\n",
        "        wdata['nationality'] = country\n",
        "        wdata['born_in'] = ''\n",
        "    # store a copy of the link's text-string for any manual corrections\n",
        "    wdata['text'] = text\n",
        "    return wdata\n",
        "  \n",
        "  def parse_bio(self, response):\n",
        "    item = response.meta['item']\n",
        "    href = response.xpath(\"//li[@id='t-wikibase']/a/@href\").extract()\n",
        "    if href:\n",
        "      request = scrapy.Request(href[0],\\\n",
        "                  callback=self.parse_wikidata,\\\n",
        "                              dont_filter=True)\n",
        "      request.meta['item'] = item\n",
        "      yield request\n",
        "  def parse_wikidata(self, response):\n",
        "    item = response.meta['item']\n",
        "    property_codes = [\n",
        "      {'name':'date_of_birth', 'code':'P569'},\n",
        "      {'name':'date_of_death', 'code':'P570'},\n",
        "      {'name':'place_of_birth', 'code':'P19', 'link':True},\n",
        "      {'name':'place_of_death', 'code':'P20', 'link':True},\n",
        "      {'name':'gender', 'code':'P21', 'link':True}\n",
        "    ]    \n",
        "    p_template = '//*[@id=\"%(code)s\"]/div[2]/div[1]/div/div[2]/div[2]/div[1]'\n",
        "    for prop in property_codes:\n",
        "      extra_html = ''\n",
        "      if prop.get('link'): # property string in <a> tag\n",
        "        extra_html = '/a'\n",
        "      sel = response.xpath(p_template%prop + extra_html + '/text()')\n",
        "      if sel:\n",
        "        item[prop['name']] = sel[0].extract()\n",
        "    yield item\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6N9qLAddoNB",
        "colab_type": "text"
      },
      "source": [
        "We will not run this comprehensive version at present since it would print out all the loggings on the screen. But we will run a really similar version in the next part where we will ignore the clumsy loggings and store what we crawl into a CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8F8JScsZ7-A"
      },
      "source": [
        "### Storing to CSV\n",
        "We may want to write the results to csv files or directly store it as RDF graphs. How could we achieve this? \n",
        "\n",
        "Hmm...\n",
        "\n",
        "We may first store the data to CSV, and then upload the CSV file to our sql server!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aZGiCQMKZ48I",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "BASE_URL = 'http://en.wikipedia.org'\n",
        "class NWinnerItem(scrapy.Item):\n",
        "  name = scrapy.Field()\n",
        "  link = scrapy.Field()\n",
        "  year = scrapy.Field()\n",
        "  category = scrapy.Field()\n",
        "  nationality = scrapy.Field()\n",
        "  gender = scrapy.Field()\n",
        "  born_in = scrapy.Field()\n",
        "  date_of_birth = scrapy.Field()\n",
        "  date_of_death = scrapy.Field()\n",
        "  place_of_birth = scrapy.Field()\n",
        "  place_of_death = scrapy.Field()\n",
        "  text = scrapy.Field()\n",
        "  \n",
        "# B Create a named spider\n",
        "class NWinnerSpiderToCsv(scrapy.Spider):\n",
        "  \"\"\" Scrapes the country and link-text of the Nobel-winners. \"\"\"\n",
        "  name = 'nwinners_list'\n",
        "  allowed_domains = ['en.wikipedia.org']\n",
        "  start_urls = [\"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country\"]\n",
        "  items = []\n",
        "  output = name+\".csv\" \n",
        "  custom_settings = {\n",
        "      'LOG_LEVEL': 'INFO',\n",
        "      'FEED_FORMAT':'csv',\n",
        "      'FEED_URI': 'nwinners_list.csv'\n",
        "    }\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    if os.path.isfile(self.output):\n",
        "      os.remove(self.output)\n",
        "    open(self.output, \"w\").close()\n",
        "  '''\n",
        "\n",
        "  \n",
        "  # C A parse method to deal with the HTTP response\n",
        "  def parse(self, response):\n",
        "    h3s = response.xpath('//h3')\n",
        "    for h3 in h3s:\n",
        "      country = h3.xpath('span[@class=\"mw-headline\"]/text()')\\\n",
        "                .extract()\n",
        "      if country:\n",
        "        winners = h3.xpath('following-sibling::ol[1]')\n",
        "        for w in winners.xpath('li'):\n",
        "          wdata = self.process_winner_li(w, country[0])\n",
        "          #pdb.set_trace()\n",
        "          request = scrapy.Request(\n",
        "            wdata['link'],\n",
        "            callback=self.parse_bio,\n",
        "            dont_filter=True)\n",
        "          #pdb.set_trace()\n",
        "          request.meta['item'] = NWinnerItem(**wdata)\n",
        "          yield request\n",
        "          #pdb.set_trace()\n",
        "    #pdb.set_trace()\n",
        "    return self.items\n",
        "  \n",
        "      \n",
        "  def process_winner_li(self, w, country=None):\n",
        "    \"\"\"\n",
        "    Process a winner's <li> tag, adding country of birth or\n",
        "    nationality, as applicable.\n",
        "    \"\"\"\n",
        "    wdata = {}\n",
        "    wdata['link'] = BASE_URL + w.xpath('a/@href').extract()[0]\n",
        "    text = ' '.join(w.xpath('descendant-or-self::text()').extract())\n",
        "    # get comma-delineated name and strip trailing white-space\n",
        "    wdata['name'] = text.split(',')[0].strip()\n",
        "    # see if there are four adjecent integers in the string text\n",
        "    year = re.findall('\\d{4}', text)\n",
        "    if year:\n",
        "      wdata['year'] = int(year[0])\n",
        "    else:\n",
        "      wdata['year'] = 0\n",
        "      print('Oops, no year in ', text)\n",
        "    category = re.findall('Physics|Chemistry|Physiology or Medicine|Literature|Peace|Economics',text)\n",
        "    if category:\n",
        "      wdata['category'] = category[0]\n",
        "    else:\n",
        "      wdata['category'] = ''\n",
        "      print('Oops, no category in ', text)\n",
        "    if country:\n",
        "      # the interesting label that represent his nationality or motherland\n",
        "      if text.find('*') != -1:\n",
        "        wdata['nationality'] = ''\n",
        "        wdata['born_in'] = country\n",
        "      else:\n",
        "        wdata['nationality'] = country\n",
        "        wdata['born_in'] = ''\n",
        "    # store a copy of the link's text-string for any manual corrections\n",
        "    wdata['text'] = text\n",
        "    #pdb.set_trace()\n",
        "    return wdata\n",
        "  \n",
        "  def parse_bio(self, response):\n",
        "    #pdb.set_trace()\n",
        "    item = response.meta['item']\n",
        "    href = response.xpath(\"//li[@id='t-wikibase']/a/@href\").extract()\n",
        "    if href:\n",
        "      request = scrapy.Request(href[0],\\\n",
        "                  callback=self.parse_wikidata,\\\n",
        "                              dont_filter=True)\n",
        "      request.meta['item'] = item\n",
        "      return request\n",
        "  def parse_wikidata(self, response):\n",
        "    #pdb.set_trace()\n",
        "    item = response.meta['item']\n",
        "    property_codes = [\n",
        "      {'name':'date_of_birth', 'code':'P569'},\n",
        "      {'name':'date_of_death', 'code':'P570'},\n",
        "      {'name':'place_of_birth', 'code':'P19', 'link':True},\n",
        "      {'name':'place_of_death', 'code':'P20', 'link':True},\n",
        "      {'name':'gender', 'code':'P21', 'link':True}\n",
        "    ]\n",
        "    # this template should be obtained by carefully examining the webpage's elements\n",
        "    p_template = '//*[@id=\"%(code)s\"]/div[2]/div/div/div[2]/div[1]/div/div[2]/div[2]/div[1]'\n",
        "    for prop in property_codes:\n",
        "      extra_html = ''\n",
        "      if prop.get('link'): # property string in <a> tag\n",
        "        extra_html = '/a'\n",
        "      \n",
        "      sel = response.xpath(p_template%prop + extra_html + '/text()')\n",
        "      #pdb.set_trace()\n",
        "      if sel:\n",
        "        item[prop['name']] = sel[0].extract()\n",
        "      else:\n",
        "        item[prop['name']] = \"\"\n",
        "    self.items.append(NWinnerItem(name=item[\"name\"], link=item[\"link\"], \n",
        "                                   year=item[\"year\"], category=item[\"category\"],\n",
        "                                  nationality=item[\"nationality\"], gender=item[\"gender\"],\n",
        "                                  born_in=item[\"born_in\"], date_of_birth=item[\"date_of_birth\"],\n",
        "                                  date_of_death=item[\"date_of_death\"],\n",
        "                                  place_of_birth=item[\"place_of_birth\"],\n",
        "                                  place_of_death=item[\"place_of_death\"],\n",
        "                                  text=item[\"text\"]))\n",
        "    return item\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QED04UTHXNrk"
      },
      "source": [
        "Now, let's try to see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zddzchmuJz3Z",
        "colab": {}
      },
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(NWinnerSpiderToCsv)\n",
        "process.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2BDMQxB3vcek"
      },
      "source": [
        "## Building Graphs\n",
        "\n",
        "The code in this part is to generate RDF graphs from datasets. For the simplicity, we would only consider the simplest case - Mapping structured data into graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPLj4T58x5H8"
      },
      "source": [
        "### Structured datasets\n",
        "\n",
        "In this part, we would build our RDF graphs from the structured datasets (i.e. CSV files). We would achieve it by applying the libraries *rdflib* and *csv*. \n",
        "\n",
        "This function would receive 3 three parameters:\n",
        "\n",
        "- filepath: the path of our CSV file\n",
        "- output : the expected path of our output file\n",
        "- output_format: the output format, including 'xml', 'n3', 'turtle', 'nt', 'pretty-xml', 'trix', 'trig' and 'nquads'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2Mh-BbKuxHm",
        "colab": {}
      },
      "source": [
        "def R2RDF(filepath, output, output_format):\n",
        "  first_Row = True\n",
        "  graph = Graph()\n",
        "  if os.path.isfile(filepath):\n",
        "    with open(filepath, encoding=\"utf-8\") as csvfile:\n",
        "      readCSV = csv.reader(csvfile,delimiter=',')\n",
        "      wiki_prefix = Namespace('https://en.wikipedia.org/wiki/')\n",
        "      for row in readCSV:\n",
        "        #print(row)\n",
        "        if first_Row == True:\n",
        "          first_Row = False\n",
        "          continue\n",
        "        born_in = Literal(row[0])\n",
        "        category = Literal(row[1])\n",
        "        date_of_birth = Literal(row[2])\n",
        "        date_of_death = Literal(row[3])\n",
        "        gender = Literal(row[4])\n",
        "        link = Literal(row[5])\n",
        "        name = Literal(row[6])\n",
        "        nationality = Literal(row[7])\n",
        "        place_of_birth = Literal(row[8])\n",
        "        place_of_death = Literal(row[9])\n",
        "        text = Literal(row[10])\n",
        "        year = Literal(row[11])\n",
        "        current_node = URIRef(link)\n",
        "        graph.add((current_node, RDF.type, FOAF.Person))\n",
        "        graph.add((current_node, FOAF.name, name))\n",
        "        graph.add((current_node, wiki_prefix.homeland, born_in))\n",
        "        #if born_in != \"\":\n",
        "        #graph.add((born_in, RDF.type, wiki_prefix.country))\n",
        "        graph.add((current_node, wiki_prefix.Course_education, category))\n",
        "        graph.add((current_node, wiki_prefix.date_of_birth,date_of_birth))\n",
        "        #graph.add((date_of_birth, RDF.type, FOAF.Date))\n",
        "        #if date_of_death != \"\":\n",
        "          #graph.add((date_of_death, RDF.type, FOAF.Date))\n",
        "        graph.add((current_node, wiki_prefix.date_of_death, date_of_death))\n",
        "        #if gender != \"\":\n",
        "        graph.add((current_node, FOAF.gender, gender))\n",
        "        graph.add((current_node, FOAF.accountServiceHomepage, link))\n",
        "        graph.add((current_node, wiki_prefix.nationality, nationality))\n",
        "        #graph.add((nationality, RDF.type, wiki_prefix.country))\n",
        "        #if place_of_birth != \"\":\n",
        "        graph.add((current_node, wiki_prefix.place_of_birth, place_of_birth))\n",
        "        #graph.add((place_of_birth, RDF.type, wiki_prefix.city))\n",
        "        #if place_of_death != \"\":\n",
        "        graph.add((current_node, wiki_prefix.place_of_death, place_of_death))\n",
        "        #graph.add((place_of_death, RDF.type, wiki_prefix.city))\n",
        "        graph.add((current_node, FOAF.depiction, text))\n",
        "        graph.add((current_node, wiki_prefix.year, year))\n",
        "    if os.path.isfile(output):\n",
        "        os.remove(output)\n",
        "    graph.serialize(destination=output, format=output_format)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSta8lijV-OY",
        "colab_type": "text"
      },
      "source": [
        "Now let's look how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h91POpQsV-Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R2RDF('nwinners_list.csv', 'nwinners_list.ttl', 'turtle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Tjcrq6V-Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R2RDF('nwinners_list.csv', 'nwinners_list.xml', 'pretty-xml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "loCdcKNLuxHo"
      },
      "source": [
        "## Sparql Querior\n",
        "\n",
        "We are writing a class that is able to achieve basic implementations of the SPQRQL queries for an RDF database. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIhw_amOV-Of",
        "colab_type": "text"
      },
      "source": [
        "### Querying with SPARQLWrapper (probably failed)\n",
        "The following functions are to be considered:\n",
        "\n",
        "- Function *init*\n",
        "- Function *Querying_database* \n",
        "\n",
        "The Fit_project and its member functions would take the inputs: \n",
        "\n",
        "- Url: the path of the RDF graph\n",
        "\n",
        "- Enum: the indication that where the *Input_stirng* is the path of an input file or the sparql request\n",
        "\n",
        "- Return_format: JSON or XML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rl5YbW3CuxHr",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "class Query_with_SPARQLWrapper:\n",
        "    File_or_Query = {\n",
        "        \"File\",\n",
        "        \"Query\",\n",
        "    }\n",
        "    Return_format = {\n",
        "        \"JSON\": JSON,\n",
        "        \"XML\": XML,\n",
        "    }\n",
        "    File_path = \"\"\n",
        "    Query_string = \"\"\n",
        "    Url = \"\"\n",
        "    Sparql = SPARQLWrapper(\"\")\n",
        "    def __init__(self, Url):\n",
        "        self.Sparql = SPARQLWrapper(Url)\n",
        "    def Querying_database(self, Enum, Input_string, Return_format):\n",
        "        if Enum == \"File\":\n",
        "            self.File_path = Input_string\n",
        "            if not os.path.isfile(self.File_path):\n",
        "                raise TypeError(self.File_path + \" does not exist\")\n",
        "            self.Query_string = open(self.File_path).read().close()\n",
        "        else:\n",
        "            self.Query_string = Input_string\n",
        "        self.Sparql.setQuery(self.Query_string)\n",
        "        #pdb.set_trace()\n",
        "        if Return_format == \"JSON\":\n",
        "            self.Sparql.setReturnFormat(JSON)\n",
        "        else:\n",
        "            self.Sparql.setReturnFormat(XML)\n",
        "        results = self.Sparql.query().convert()\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrILb-wyV-Oj",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62rLYv2jV-Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fp = Fit_project(\"https://raw.githubusercontent.com/JinboCi/Sparql_tutorials/master/nwinner_list.xml\")\n",
        "print(Fp.Querying_database(\"Query\", \n",
        "                           \"\"\"SELECT ?subject ?predicate \n",
        "WHERE {\n",
        "  ?subject ?predicate \"Chicago\"\n",
        "}\n",
        "LIMIT 25\"\"\", \n",
        "                           \"JSON\").decode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a57u1-zoV-On",
        "colab_type": "text"
      },
      "source": [
        "### Rdflib method\n",
        "\n",
        "OMG, we have failed to use SPARQLWrapper for querying. Sadly but hopefully, there is a another way!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1DWor24V-Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Rdflib_method(filepath, inputstring):\n",
        "    graph = rdflib.Graph()\n",
        "    graph.parse(filepath, format='xml')\n",
        "    qres = graph.query(inputstring)\n",
        "    for row in qres:\n",
        "        print(row)\n",
        "    return qres"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKMDTjbIV-Oq",
        "colab_type": "text"
      },
      "source": [
        "How it works?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEhEVS9dV-Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Rdflib_method(\"https://raw.githubusercontent.com/JinboCi/Sparql_tutorials/master/nwinner_list.xml\", \n",
        "             \"\"\"\n",
        "             PREFIX  foaf: <http://xmlns.com/foaf/0.1/>\n",
        "SELECT ?object \n",
        "WHERE {\n",
        "  ?subject ?predicate \"Tibet\".\n",
        "  ?subject foaf:name ?object\n",
        "}\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RryE7S90hkWZ",
        "colab_type": "text"
      },
      "source": [
        "Yes! We build this knowledge graph system successfully!"
      ]
    }
  ]
}